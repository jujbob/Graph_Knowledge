{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7d0aa927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchkge in /home/aailab/.local/lib/python3.8/site-packages (0.17.5)\n",
      "Requirement already satisfied: tqdm>=4.64 in /home/aailab/.local/lib/python3.8/site-packages (from torchkge) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.2.0 in /home/aailab/.local/lib/python3.8/site-packages (from torchkge) (1.10.0+cu111)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/aailab/.local/lib/python3.8/site-packages (from torchkge) (1.23.4)\n",
      "Requirement already satisfied: pandas>=1.4 in /home/aailab/.local/lib/python3.8/site-packages (from torchkge) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions in /home/aailab/.local/lib/python3.8/site-packages (from torch>=1.2.0->torchkge) (4.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aailab/.local/lib/python3.8/site-packages (from pandas>=1.4->torchkge) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/aailab/.local/lib/python3.8/site-packages (from pandas>=1.4->torchkge) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=1.4->torchkge) (1.14.0)\n",
      "Requirement already satisfied: soynlp in /home/aailab/.local/lib/python3.8/site-packages (0.0.493)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /home/aailab/.local/lib/python3.8/site-packages (from soynlp) (1.23.4)\n",
      "Requirement already satisfied: psutil>=5.0.1 in /home/aailab/.local/lib/python3.8/site-packages (from soynlp) (5.9.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/aailab/.local/lib/python3.8/site-packages (from soynlp) (0.23.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/aailab/.local/lib/python3.8/site-packages (from soynlp) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/aailab/.local/lib/python3.8/site-packages (from scikit-learn>=0.20.0->soynlp) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/aailab/.local/lib/python3.8/site-packages (from scikit-learn>=0.20.0->soynlp) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchkge\n",
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "967f3113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from torchkge.models import ConvKBModel\n",
    "import pickle \n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm \n",
    "from kge_train import train_kge\n",
    "from torchkge.utils.datasets import load_fb15k\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from soynlp.hangle import levenshtein\n",
    "from soynlp.hangle import jamo_levenshtein\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "946c9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cbb6c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "805a5143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"./datasets/20220920_csv/병원_triple.csv\", encoding='cp949')\n",
    "#df[530:550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e600eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"./datasets/04 경찰서 병원 소방서/소방서_triple.csv\", encoding='cp949')\n",
    "FOLDER_NAME = \"datasets/20220920_csv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4f71be8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the file:  ./datasets/20220920_csv/ds_r_f15048634_202111221427(relation)07.csv\n",
      "Reading the file:  ./datasets/20220920_csv/ds_r_f15048634_202111221427(relation)06.csv\n",
      "Reading the file:  ./datasets/20220920_csv/ds_r_f15048634_202111221427(relation)02.csv\n",
      "Reading the file:  ./datasets/20220920_csv/02 해운대_배수펌프(3075794).csv\n",
      "Reading the file:  ./datasets/20220920_csv/ds_r_f15048634_202111221427(relation)05.csv\n",
      "Reading the file:  ./datasets/20220920_csv/ds_r_f15048634_202111221427(relation).xlsx\n",
      "읽으려는 파일의 포멧을 확인해야함, 파일명:  ./datasets/20220920_csv/ds_r_f15048634_202111221427(relation).xlsx\n",
      "Reading the file:  ./datasets/20220920_csv/ds_r_f15048634_202111221427(relation)04.csv\n",
      "Reading the file:  ./datasets/20220920_csv/병원_triple.csv\n",
      "Reading the file:  ./datasets/20220920_csv/ds_r_f15048634_202111221427(relation)03.csv\n",
      "Reading the file:  ./datasets/20220920_csv/00 general_triple.csv\n",
      "Reading the file:  ./datasets/20220920_csv/소방서_triple.csv\n",
      "Reading the file:  ./datasets/20220920_csv/01 부산_유수지(15054723).csv\n",
      "Reading the file:  ./datasets/20220920_csv/ds_r_f15048634_202111221427(relation)01.csv\n",
      "Reading the file:  ./datasets/20220920_csv/ds_r_f15048634_202111221427(relation)08.csv\n",
      "Reading the file:  ./datasets/20220920_csv/ds_r_f15048634_202111221427(relation)09.csv\n",
      "Reading the file:  ./datasets/20220920_csv/경찰서_triple.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  kg_full = kg_full.append(a_df, ignore_index=True)\n",
      "/tmp/ipykernel_2808834/4084913395.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full = df_full.append(tmp_df, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "path_to_file_folder = HOME_DIR+FOLDER_NAME\n",
    "meta_data= [\"file_name\", \"colum_name\", \"value_list\"] #the meta-data should be extented based on EDA on the datasets\n",
    "df_full = pd.DataFrame(columns=meta_data)\n",
    "meta_data= [\"head\", \"relation\", \"tail\", \"file_name\"] #the meta-data should be extented based on EDA on the datasets\n",
    "kg_full = pd.DataFrame(columns=meta_data)\n",
    "\n",
    "for file_name in [file for file in os.listdir(path_to_file_folder)]:\n",
    "    file_name = file_name\n",
    "    a_file = HOME_DIR+FOLDER_NAME+file_name\n",
    "    print(\"Reading the file: \", a_file)\n",
    "    \n",
    "    try:\n",
    "        a_df = pd.DataFrame()\n",
    "        if \".csv\" in file_name:\n",
    "            a_df = pd.read_csv(a_file, encoding='cp949', names=meta_data, header=None)\n",
    "            a_df[\"file_name\"] = file_name\n",
    "        elif \".xlsx\" in file_name:\n",
    "            a_df = pd.read_excel(a_file, names=meta_data, header=None)\n",
    "            a_df[\"file_name\"] = file_name\n",
    "        else:\n",
    "            continue\n",
    "        kg_full = kg_full.append(a_df, ignore_index=True)\n",
    "        \n",
    "\n",
    "        file_name_list = []\n",
    "        colum_name_list = []\n",
    "        value_list = []\n",
    "        for col in a_df.columns:\n",
    "            col_name = col\n",
    "            col_values = list(a_df[col_name].values)\n",
    "            file_name_list.append(file_name)\n",
    "            colum_name_list.append(col_name)\n",
    "            value_list.append(col_values)\n",
    "\n",
    "        tmp_df = pd.DataFrame({\"file_name\": file_name_list,\n",
    "                            \"colum_name\": colum_name_list,\n",
    "                            \"value_list\": value_list})\n",
    "\n",
    "        df_full = df_full.append(tmp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        print(\"읽으려는 파일의 포멧을 확인해야함, 파일명: \", a_file,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "44285896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>tail</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7633</td>\n",
       "      <td>start_time</td>\n",
       "      <td>0</td>\n",
       "      <td>ds_r_f15048634_202111221427(relation)07.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7634</td>\n",
       "      <td>start_time</td>\n",
       "      <td>0</td>\n",
       "      <td>ds_r_f15048634_202111221427(relation)07.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7635</td>\n",
       "      <td>start_time</td>\n",
       "      <td>0</td>\n",
       "      <td>ds_r_f15048634_202111221427(relation)07.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7636</td>\n",
       "      <td>start_time</td>\n",
       "      <td>0</td>\n",
       "      <td>ds_r_f15048634_202111221427(relation)07.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7637</td>\n",
       "      <td>start_time</td>\n",
       "      <td>0</td>\n",
       "      <td>ds_r_f15048634_202111221427(relation)07.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10438</th>\n",
       "      <td>광남지구대</td>\n",
       "      <td>latitude</td>\n",
       "      <td>35.14537016</td>\n",
       "      <td>경찰서_triple.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10439</th>\n",
       "      <td>망미2치안센터</td>\n",
       "      <td>latitude</td>\n",
       "      <td>35.1717912</td>\n",
       "      <td>경찰서_triple.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10440</th>\n",
       "      <td>부산사상경찰서</td>\n",
       "      <td>latitude</td>\n",
       "      <td>35.17452651</td>\n",
       "      <td>경찰서_triple.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10441</th>\n",
       "      <td>부산기장경찰서</td>\n",
       "      <td>latitude</td>\n",
       "      <td>35.32481808</td>\n",
       "      <td>경찰서_triple.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10442</th>\n",
       "      <td>장안파출소</td>\n",
       "      <td>latitude</td>\n",
       "      <td>35.32478276</td>\n",
       "      <td>경찰서_triple.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10443 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          head    relation         tail  \\\n",
       "0         7633  start_time            0   \n",
       "1         7634  start_time            0   \n",
       "2         7635  start_time            0   \n",
       "3         7636  start_time            0   \n",
       "4         7637  start_time            0   \n",
       "...        ...         ...          ...   \n",
       "10438    광남지구대    latitude  35.14537016   \n",
       "10439  망미2치안센터    latitude   35.1717912   \n",
       "10440  부산사상경찰서    latitude  35.17452651   \n",
       "10441  부산기장경찰서    latitude  35.32481808   \n",
       "10442    장안파출소    latitude  35.32478276   \n",
       "\n",
       "                                         file_name  \n",
       "0      ds_r_f15048634_202111221427(relation)07.csv  \n",
       "1      ds_r_f15048634_202111221427(relation)07.csv  \n",
       "2      ds_r_f15048634_202111221427(relation)07.csv  \n",
       "3      ds_r_f15048634_202111221427(relation)07.csv  \n",
       "4      ds_r_f15048634_202111221427(relation)07.csv  \n",
       "...                                            ...  \n",
       "10438                               경찰서_triple.csv  \n",
       "10439                               경찰서_triple.csv  \n",
       "10440                               경찰서_triple.csv  \n",
       "10441                               경찰서_triple.csv  \n",
       "10442                               경찰서_triple.csv  \n",
       "\n",
       "[10443 rows x 4 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "73abe64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    kge_n_epoch = 100\n",
    "    kge_lr = 1e-4\n",
    "    kge_batch = 64\n",
    "    kge_margin = 0.5\n",
    "    kge_conv_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5e7ee74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9c0265d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kg_full[[\"head\", \"relation\", \"tail\"]].to_csv(\"triple_kb2.csv\")\n",
    "kg_full = kg_full.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e2c2e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_emb_path = './kge_save'\n",
    "os.makedirs(kb_emb_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e56ee83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchkge.utils.datasets import load_fb15k\n",
    "#kg, kg_val, kg_test = load_fb15k()\n",
    "#\n",
    "#model_convKB = ConvKBModel(256,\n",
    "#                            3,\n",
    "#                            kg.n_ent,\n",
    "#                            kg.n_rel,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "93396906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originally counts 10443\n",
      "\n",
      "user nunique 1250, item nunique 2457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100 | mean loss: 4.59009: 100%|██████████| 100/100 [00:42<00:00,  2.34epoch/s]\n"
     ]
    }
   ],
   "source": [
    "kg, model_convKB = train_kge(kg_full[[\"head\", \"relation\", \"tail\"]],\n",
    "                             epochs=config.kge_n_epoch, \n",
    "                                lr=config.kge_lr, \n",
    "                                batch_size=config.kge_batch,\n",
    "                                margin=config.kge_margin,\n",
    "                                conv_size=config.kge_conv_size\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7fafe10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3705, 256)\n"
     ]
    }
   ],
   "source": [
    "emb_entity_ = model_convKB.get_embeddings()[0].detach().cpu().numpy()\n",
    "emb_rel_ = model_convKB.get_embeddings()[1].detach().cpu().numpy()\n",
    "\n",
    "ent_emb_dim = len(emb_entity_[0])\n",
    "ent_vocab={}\n",
    "ent_vocab['<unk>'] = 0\n",
    "ent_vocab['<pad>'] = 1\n",
    "ent_vocab.update({word: i+2 for word, i in kg.ent2ix.items()})\n",
    "ent_ids = {i: word for word, i in ent_vocab.items()}\n",
    "\n",
    "pad_emb_npa = np.zeros((1, ent_emb_dim))   #embedding for '<pad>' token.\n",
    "unk_emb_npa = np.zeros((1, ent_emb_dim))   #embedding for '<unk>' token.\n",
    "\n",
    "ent_embs_npa = np.vstack((pad_emb_npa,unk_emb_npa,emb_entity_))\n",
    "print(ent_embs_npa.shape)\n",
    "ent_emb = torch.nn.Embedding.from_pretrained(torch.from_numpy(ent_embs_npa).float(), freeze=False)\n",
    "\n",
    "with open('ent_vocab_npa.npy','wb') as f:\n",
    "    np.save(f,ent_vocab)\n",
    "\n",
    "with open('ent_embs_npa.npy','wb') as f:\n",
    "    np.save(f,ent_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0ebffeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3495e-02,  2.7541e-02, -1.4692e-02,  2.0852e-02, -5.6799e-03,\n",
      "         -1.0114e-02,  6.6786e-03, -1.5817e-02,  1.3240e-02,  2.5674e-02,\n",
      "          1.9184e-02,  5.0760e-03,  1.0856e-02, -6.8166e-03, -2.4069e-02,\n",
      "          1.1869e-02, -4.7383e-03, -2.3067e-02,  3.5151e-02, -1.6735e-03,\n",
      "          2.8682e-03,  1.4233e-02,  1.6200e-02, -5.2617e-03,  2.8609e-02,\n",
      "         -2.6667e-03, -3.2197e-02, -3.7139e-03,  1.2010e-02, -7.6127e-04,\n",
      "         -3.1458e-02, -1.9014e-03,  2.7984e-02,  3.5050e-02,  1.6103e-02,\n",
      "          1.8406e-02, -1.9635e-02, -9.8370e-03,  2.9551e-02,  2.7809e-02,\n",
      "          8.2627e-03,  5.5127e-03, -1.5847e-02, -3.5770e-02, -5.8009e-03,\n",
      "         -7.3087e-03,  3.2922e-02, -2.6567e-02,  9.9924e-03, -5.5123e-03,\n",
      "          4.5479e-03, -1.7145e-02,  1.5799e-02,  7.1761e-03, -1.3002e-02,\n",
      "          1.8460e-02,  5.3515e-03, -2.5622e-02,  2.4684e-02,  2.0875e-03,\n",
      "         -5.6280e-03,  3.3184e-02,  1.1812e-02, -2.4457e-02, -1.3043e-02,\n",
      "          1.3301e-02,  2.3331e-02,  2.8340e-02,  7.4099e-03, -2.7941e-02,\n",
      "         -2.1891e-03, -1.9050e-02,  3.1835e-02, -9.3673e-03, -3.3063e-02,\n",
      "         -6.3556e-03, -6.0236e-03,  2.5249e-02,  2.8730e-02, -4.5799e-03,\n",
      "          4.5427e-03, -1.1931e-02, -2.2671e-02,  3.7297e-02,  9.7060e-03,\n",
      "         -2.2014e-02, -4.5018e-03, -3.5168e-03,  2.8660e-03,  7.9185e-03,\n",
      "          1.6601e-02,  2.9658e-02,  2.4335e-02, -6.0374e-03, -3.6533e-03,\n",
      "         -1.5580e-02,  1.1936e-02,  1.2854e-03, -1.6079e-02,  2.5912e-02,\n",
      "         -2.1839e-02, -2.0838e-02,  4.2934e-03,  1.2752e-02,  1.9568e-02,\n",
      "          4.8815e-03,  3.1167e-03,  6.9952e-03, -1.0635e-02, -4.5843e-03,\n",
      "          8.8435e-03, -2.2905e-02, -6.4200e-03, -8.2594e-03,  2.3027e-02,\n",
      "         -1.6065e-02, -1.9437e-02,  6.8375e-03,  1.8520e-02, -2.2760e-02,\n",
      "         -2.0960e-02, -2.9181e-02, -1.0653e-02,  2.3907e-02,  9.9307e-03,\n",
      "          1.0425e-02,  2.8055e-02, -4.9141e-03,  5.5787e-03, -1.8274e-02,\n",
      "         -7.2442e-03,  2.5370e-02,  1.1888e-02, -1.8459e-02, -2.5182e-02,\n",
      "         -1.0820e-02, -2.6770e-03, -2.4686e-02, -1.9846e-02,  2.9599e-03,\n",
      "          5.9083e-03,  2.5884e-02,  1.3288e-02,  7.2195e-03, -7.8671e-03,\n",
      "         -1.2355e-02,  2.3590e-02, -6.1415e-04,  1.7022e-02, -9.8843e-04,\n",
      "         -2.1289e-03, -4.9924e-03, -4.5283e-03,  2.7589e-03, -1.9851e-02,\n",
      "         -2.7163e-02,  3.5912e-02,  4.8683e-03, -1.1523e-02, -4.1104e-02,\n",
      "         -5.2625e-03,  3.2931e-02, -9.1117e-03,  3.6307e-02,  3.3751e-02,\n",
      "          5.1677e-03,  4.9965e-04, -1.4025e-02,  3.1663e-02,  2.5907e-02,\n",
      "         -1.5515e-02, -1.7727e-02,  3.8054e-02, -2.1123e-03, -5.5308e-03,\n",
      "          8.1743e-03,  7.1908e-03, -2.4934e-02,  6.9034e-03, -7.0578e-03,\n",
      "         -1.3191e-02,  3.8494e-03,  2.6720e-02,  2.4255e-02, -3.7525e-02,\n",
      "          3.5233e-02, -2.8466e-02, -1.8638e-02, -6.4813e-03, -6.1971e-03,\n",
      "          2.1638e-02, -9.0682e-03, -2.0271e-02,  2.0364e-02,  1.2580e-02,\n",
      "         -3.5759e-02,  3.4840e-03, -1.3191e-02,  1.6465e-02,  3.5638e-02,\n",
      "          2.6206e-02,  4.3636e-03,  1.2786e-04, -1.8921e-02, -2.4921e-04,\n",
      "          5.6085e-03,  8.0355e-03,  3.5787e-03,  2.4915e-02,  2.2580e-02,\n",
      "          9.1677e-03, -1.1533e-02,  2.0064e-03,  1.2011e-02, -1.8115e-02,\n",
      "          2.9312e-02,  2.1756e-04, -3.8157e-02,  3.5791e-03,  3.5683e-02,\n",
      "         -1.9590e-02, -2.8881e-02,  1.1151e-02, -6.4927e-03,  2.0148e-03,\n",
      "          2.2818e-02,  9.2734e-04,  1.3825e-02, -1.6031e-02,  3.5153e-02,\n",
      "         -9.1909e-03,  1.7626e-05, -1.1822e-02, -3.7742e-02,  5.3006e-03,\n",
      "          1.3719e-02,  2.7844e-02, -2.5742e-02, -4.5572e-03, -3.3672e-02,\n",
      "         -9.9099e-03, -1.8816e-02,  1.3236e-02, -2.1084e-02, -1.7790e-03,\n",
      "         -1.7029e-02,  2.4798e-02,  6.2286e-03, -2.7063e-02,  3.3315e-02,\n",
      "          6.2761e-03,  2.1597e-02,  2.3727e-02,  1.0531e-03, -1.9939e-02,\n",
      "          2.7322e-02],\n",
      "        [ 6.2674e-03,  1.0997e-02, -2.3011e-02,  1.5556e-02, -2.1407e-02,\n",
      "         -6.2980e-03,  1.1177e-02, -2.4834e-02, -1.1544e-02, -2.9302e-02,\n",
      "          1.7747e-02, -3.5818e-03, -5.8765e-03,  3.7325e-04, -3.2310e-03,\n",
      "          1.5651e-02,  5.1300e-03, -7.5300e-03, -9.8059e-03, -6.3079e-03,\n",
      "         -7.0546e-03,  6.3298e-03,  2.7523e-02, -7.5028e-05,  1.6881e-03,\n",
      "          6.9506e-03,  4.1782e-03,  2.3391e-03,  7.0331e-03,  2.9578e-03,\n",
      "          1.0079e-02,  2.5622e-02,  2.4645e-03,  1.4899e-02,  2.7577e-02,\n",
      "         -1.7307e-03, -5.5415e-03, -6.7249e-03, -2.1926e-03,  4.8869e-03,\n",
      "          1.8116e-02, -6.9261e-03, -4.5335e-03,  1.5672e-02,  6.9426e-03,\n",
      "          7.5246e-03, -6.5174e-03, -9.8724e-03, -5.5082e-03,  9.8217e-03,\n",
      "         -1.4794e-02, -5.7891e-03, -1.3822e-03,  1.1890e-03,  7.9259e-04,\n",
      "          1.4993e-02, -2.0923e-02, -1.4529e-02,  6.5640e-03,  2.1991e-02,\n",
      "          5.6277e-03, -6.0867e-03,  5.1279e-03,  2.4288e-03, -3.9464e-03,\n",
      "         -7.5105e-03,  2.2349e-02, -2.5329e-02, -7.1702e-04,  1.9961e-03,\n",
      "         -4.7951e-03,  5.8024e-03,  1.5738e-02,  7.2829e-03,  5.5010e-03,\n",
      "          3.6564e-03, -7.4371e-03,  6.2279e-03, -3.0494e-02,  1.5476e-03,\n",
      "         -2.6350e-02, -1.5554e-02, -1.1854e-02,  9.7120e-04, -6.1596e-03,\n",
      "          6.3861e-03, -5.7375e-03,  2.6136e-02,  1.7672e-02, -1.1222e-02,\n",
      "         -2.3611e-02, -2.0907e-04,  1.6281e-02,  1.0272e-02,  8.1089e-03,\n",
      "          8.4235e-03, -7.9713e-03,  2.6449e-02,  5.3689e-03,  9.4455e-03,\n",
      "         -6.1252e-03,  2.1419e-02, -7.2816e-03,  6.2169e-03,  4.0824e-03,\n",
      "         -4.7642e-03,  1.1233e-03, -1.2911e-02, -4.5941e-03,  1.6852e-02,\n",
      "          1.2776e-02,  5.8412e-03, -7.4146e-03,  8.3129e-03,  1.4551e-03,\n",
      "          3.6661e-03,  4.4813e-03,  1.7543e-02, -6.8571e-03, -1.6201e-03,\n",
      "         -1.5791e-02, -7.1618e-03,  7.0642e-03, -1.8542e-02, -1.3302e-02,\n",
      "          7.4887e-03, -1.5990e-02,  6.7411e-03, -6.9508e-03,  9.9193e-04,\n",
      "         -1.0752e-02, -5.4078e-03,  1.1937e-02,  7.7049e-03,  1.7104e-02,\n",
      "         -9.0465e-03,  2.0996e-02, -5.5740e-03,  1.0543e-02, -5.5424e-03,\n",
      "         -4.5239e-03, -1.2822e-02, -6.4142e-03,  4.7698e-03,  1.1584e-02,\n",
      "         -6.5764e-03,  2.5111e-02,  2.1445e-03, -2.4021e-02, -2.7802e-02,\n",
      "          1.2778e-02,  2.2003e-02, -2.2453e-02,  2.2551e-02,  1.6867e-02,\n",
      "         -2.9226e-02, -5.4181e-04,  1.4428e-02, -1.2788e-02, -1.2720e-02,\n",
      "         -7.0702e-03,  1.4409e-02,  1.3518e-02, -2.8421e-02, -6.5763e-03,\n",
      "         -1.9719e-02, -2.3732e-02, -8.7240e-03, -5.9761e-03, -2.1719e-02,\n",
      "         -7.1722e-03, -5.8813e-03, -8.4181e-03, -8.1479e-03, -1.3372e-02,\n",
      "         -1.1545e-02,  1.5985e-02, -8.5796e-03, -1.3501e-02,  3.3007e-03,\n",
      "         -7.1971e-03, -5.7984e-03, -1.0572e-02, -8.7211e-03,  3.3557e-03,\n",
      "         -1.4767e-02, -9.9007e-03,  1.4128e-02, -5.8418e-03, -2.3739e-03,\n",
      "          6.5806e-03, -1.4914e-03,  1.4682e-02, -5.3906e-03, -4.4097e-04,\n",
      "          1.1873e-02, -9.2083e-03,  3.9504e-03,  5.6845e-03, -6.4883e-03,\n",
      "          1.5378e-02,  6.7065e-03, -3.3307e-02,  2.2195e-02, -5.2408e-03,\n",
      "         -6.0342e-03, -8.9258e-03, -6.1419e-03, -1.3029e-02, -1.5807e-02,\n",
      "          5.3546e-03, -9.1553e-03, -4.1201e-03,  6.1158e-03,  2.2578e-02,\n",
      "         -2.2295e-02, -6.0229e-03,  6.5687e-03, -3.7855e-03, -2.3908e-02,\n",
      "         -3.4382e-05, -1.1119e-02, -6.5097e-03, -2.4368e-03, -1.7893e-03,\n",
      "          3.8193e-03, -6.1184e-03,  7.2342e-03,  6.3210e-03,  6.1240e-03,\n",
      "         -5.3845e-03,  2.2237e-02,  1.5964e-02,  2.0617e-02,  1.0245e-02,\n",
      "         -6.7973e-03, -2.1588e-02, -5.3172e-03, -1.9642e-03, -1.0015e-02,\n",
      "         -7.8793e-03,  1.1105e-02, -2.2806e-02,  2.4398e-02,  6.6515e-03,\n",
      "         -2.9935e-03, -6.5464e-03, -4.1137e-03,  5.2137e-03,  1.2022e-02,\n",
      "         -5.6815e-03,  5.4464e-03, -3.7732e-03,  1.6428e-02,  9.8205e-03,\n",
      "          1.2313e-02]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 0.1079,  0.0907, -0.0982, -0.0897,  0.0776, -0.0759, -0.1014, -0.0949,\n",
      "         -0.0758, -0.0813,  0.0729, -0.0849, -0.0618,  0.0837, -0.0640,  0.0811,\n",
      "          0.0838, -0.0659, -0.0673, -0.0703, -0.0633,  0.0811,  0.0724,  0.0841,\n",
      "         -0.0588,  0.0867,  0.0802,  0.0626, -0.0870, -0.0628,  0.0612,  0.0642,\n",
      "          0.1051, -0.0710,  0.0916, -0.0932, -0.0818, -0.0594, -0.0824, -0.1066,\n",
      "          0.0769, -0.1013, -0.1043,  0.1062,  0.0837,  0.1020, -0.0795,  0.1097,\n",
      "         -0.0749,  0.0803, -0.1096, -0.0748,  0.1071,  0.0656, -0.0736, -0.0792,\n",
      "         -0.0818, -0.0677,  0.0815,  0.1065,  0.0802, -0.0510,  0.0746,  0.0935,\n",
      "         -0.0844, -0.1033,  0.0951, -0.0803,  0.0778,  0.0799, -0.0665, -0.0812,\n",
      "         -0.1029,  0.0782,  0.0811, -0.0802, -0.0908,  0.0793, -0.0679, -0.1113,\n",
      "         -0.0649,  0.0788, -0.0830, -0.0969, -0.0788,  0.0812, -0.0903,  0.0888,\n",
      "         -0.0900, -0.0505,  0.0888, -0.0635,  0.0631, -0.0756,  0.0979,  0.1053,\n",
      "         -0.0699,  0.0983,  0.0979,  0.0725, -0.0748,  0.0793, -0.0588,  0.0570,\n",
      "          0.0805, -0.0792, -0.0958, -0.0736,  0.0829,  0.0533, -0.0948,  0.0892,\n",
      "          0.0746,  0.0621,  0.0822, -0.0820,  0.0804, -0.0823, -0.0950, -0.0762,\n",
      "          0.0614,  0.0719, -0.0954,  0.0799,  0.1013,  0.0878, -0.0692,  0.0934,\n",
      "         -0.0693,  0.0700,  0.0547, -0.0994, -0.0855,  0.0930,  0.0666,  0.0805,\n",
      "          0.0846, -0.0602,  0.0562, -0.0764, -0.0532,  0.0783, -0.0805,  0.0955,\n",
      "          0.0704, -0.0969,  0.0601, -0.0767, -0.0833, -0.0919, -0.1071,  0.0987,\n",
      "         -0.1039,  0.0616,  0.0775, -0.0806, -0.0589, -0.0784, -0.0912,  0.0563,\n",
      "          0.0829, -0.0755, -0.0852, -0.0536, -0.0557, -0.0757, -0.0859,  0.0845,\n",
      "         -0.0995, -0.0828, -0.0657, -0.0641, -0.0979, -0.0982,  0.0793,  0.0835,\n",
      "          0.0800, -0.0931, -0.0875,  0.0504, -0.0693, -0.0879, -0.1031, -0.0754,\n",
      "          0.0582, -0.0535,  0.0575,  0.0628, -0.0987,  0.0610,  0.0584,  0.0737,\n",
      "          0.0629, -0.1053, -0.0898,  0.0804, -0.0549, -0.0767,  0.0654, -0.0789,\n",
      "         -0.0720,  0.1137,  0.0054,  0.1105, -0.0947, -0.0574, -0.0683, -0.0850,\n",
      "          0.0698, -0.0637,  0.0608, -0.0634,  0.0814,  0.0835,  0.0911, -0.0766,\n",
      "         -0.0838,  0.1079,  0.0672, -0.1031, -0.0588,  0.0805, -0.1088, -0.1098,\n",
      "          0.0658,  0.0598, -0.0910, -0.0798,  0.0766, -0.0879, -0.0732,  0.1102,\n",
      "         -0.0994,  0.0751, -0.0924, -0.0558,  0.0602, -0.0879, -0.0536,  0.0826,\n",
      "         -0.0604, -0.0679, -0.0794,  0.0606,  0.0987, -0.0683, -0.0943, -0.0812,\n",
      "          0.0615, -0.0790, -0.0832,  0.0955, -0.0978,  0.0899,  0.0512,  0.0802]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(ent_emb(torch.LongTensor([10, 15]))) \n",
    "ids = ent_vocab[\"부곡지구대\"]\n",
    "print(ent_emb(torch.LongTensor([ids])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "58a184b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3018, 2912, 3278, 2862, 2921, 3263, 2893, 3312, 2928, 3202, 3161, 3159,\n",
       "        3166, 3572, 3171, 2965, 3306, 3413, 3285, 2993, 3563, 3674, 3269, 2846,\n",
       "        2970, 3310, 3164, 3308, 3173, 2864, 3243, 3185, 2888, 3248, 3163, 2920,\n",
       "        3017, 2852, 2984, 2858, 3661, 2840, 2849, 3250, 3313, 3567,  363, 3128,\n",
       "        3157, 2978, 3637, 3638,  976, 2949, 2889, 3326, 3153, 2821, 2973, 3165,\n",
       "        2929, 2983, 3587, 3019, 3302, 3152, 3585, 2860, 3464, 2822, 3253, 3701,\n",
       "        3016, 2881, 2861, 3634, 2979, 2863, 3148, 3314, 3149, 3011, 2823, 3247,\n",
       "        3158, 2977, 3571, 3244, 2969, 3415, 3439, 3409, 2966, 3251, 3373, 2986,\n",
       "        2609, 3283, 3411, 3414])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_name=\"부곡지구대\"\n",
    "e1_ids = ent_vocab[ent_name]\n",
    "e1_torch = ent_emb(torch.LongTensor([e1_ids]))\n",
    "e2_ids = np.arange(0, len(ent_vocab))\n",
    "e2_torch = ent_emb(torch.LongTensor([e2_ids])).squeeze()\n",
    "similarity = cosine_similarity(e1_torch, e2_torch)\n",
    "s_value, s_ids = torch.topk(similarity, 100)\n",
    "s_ids = s_ids.squeeze()\n",
    "s_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "95718627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3018)\n",
      "부곡지구대\n",
      "tensor(2912)\n",
      "녹산파출소\n",
      "tensor(3278)\n",
      "신평파출소\n",
      "tensor(2862)\n",
      "구덕지구대\n",
      "tensor(2921)\n",
      "다대해양파출소\n",
      "tensor(3263)\n",
      "수성지구대\n",
      "tensor(2893)\n",
      "낙민파출소\n",
      "tensor(3312)\n",
      "온천3파출소\n",
      "tensor(2928)\n",
      "대연치안센터\n",
      "tensor(3202)\n",
      "사직지구대\n",
      "tensor(3161)\n",
      "부산사상경찰서\n",
      "tensor(3159)\n",
      "부산북부경찰서\n",
      "tensor(3166)\n",
      "부산연제경찰서\n",
      "tensor(3572)\n",
      "충렬지구대\n",
      "tensor(3171)\n",
      "부산지방경찰청경찰특공대\n",
      "tensor(2965)\n",
      "동래봉생병원\n",
      "tensor(3306)\n",
      "영선2치안센터\n",
      "tensor(3413)\n",
      "좌3치안센터\n",
      "tensor(3285)\n",
      "연산119안전센터\n",
      "tensor(2993)\n",
      "반송파출소\n",
      "tensor(3563)\n",
      "창선119안전센터\n",
      "tensor(3674)\n",
      "해운대경찰서\n",
      "tensor(3269)\n",
      "수영삼성병원\n",
      "tensor(2846)\n",
      "경희병원\n",
      "tensor(2970)\n",
      "동의병원\n",
      "tensor(3310)\n",
      "온종합병원\n",
      "tensor(3164)\n",
      "부산성소병원\n",
      "tensor(3308)\n",
      "영주파출소\n",
      "tensor(3173)\n",
      "부산지방경찰청제2기동대\n",
      "tensor(2864)\n",
      "구포부민병원\n",
      "tensor(3243)\n",
      "서울아동병원\n",
      "tensor(3185)\n",
      "부산진소방서\n",
      "tensor(2888)\n",
      "기장소방서\n",
      "tensor(3248)\n",
      "성지지구대\n",
      "tensor(3163)\n",
      "부산성모병원\n",
      "tensor(2920)\n",
      "다대자연병원\n",
      "tensor(3017)\n",
      "봉생병원\n",
      "tensor(2852)\n",
      "공항파출소\n",
      "tensor(2984)\n",
      "문현지구대\n",
      "tensor(2858)\n",
      "광혜병원\n",
      "tensor(3661)\n",
      "해동병원\n",
      "tensor(2840)\n",
      "개금1치안센터\n",
      "tensor(2849)\n",
      "고신의료원\n",
      "tensor(3250)\n",
      "세웅병원\n",
      "tensor(3313)\n",
      "온천지구대\n",
      "tensor(3567)\n",
      "청학119안전센터\n",
      "tensor(363)\n",
      "119수상구조대\n",
      "tensor(3128)\n",
      "부산강서경찰서\n",
      "tensor(3157)\n",
      "부산보훈병원\n",
      "tensor(2978)\n",
      "망미119안전센터\n",
      "tensor(3637)\n",
      "프라임병원\n",
      "tensor(3638)\n",
      "하나병원\n",
      "tensor(976)\n",
      "21세기라파병원\n",
      "tensor(2949)\n",
      "동남권원자력병원\n",
      "tensor(2889)\n",
      "길정병원\n",
      "tensor(3326)\n",
      "우리들병원\n",
      "tensor(3153)\n",
      "부산동래경찰서\n",
      "tensor(2821)\n",
      "가덕도보건지소\n",
      "tensor(2973)\n",
      "마더즈병원\n",
      "tensor(3165)\n",
      "부산센텀병원\n",
      "tensor(2929)\n",
      "대청파출소\n",
      "tensor(2983)\n",
      "명지파출소\n",
      "tensor(3587)\n",
      "태종대요양병원\n",
      "tensor(3019)\n",
      "부민병원\n",
      "tensor(3302)\n",
      "영도병원\n",
      "tensor(3152)\n",
      "부산대학교병원\n",
      "tensor(3585)\n",
      "큐병원\n",
      "tensor(2860)\n",
      "괴정3동치안센터\n",
      "tensor(3464)\n",
      "지사파출소\n",
      "tensor(2822)\n",
      "가덕도파출소\n",
      "tensor(3253)\n",
      "송도지구대\n",
      "tensor(3701)\n",
      "화명일신기독병원\n",
      "tensor(3016)\n",
      "봉래치안센터\n",
      "tensor(2881)\n",
      "기장119구조대\n",
      "tensor(2861)\n",
      "괴정병원\n",
      "tensor(3634)\n",
      "패밀리병원\n",
      "tensor(2979)\n",
      "망미2치안센터\n",
      "tensor(2863)\n",
      "구평치안센터\n",
      "tensor(3148)\n",
      "부산광역시의료원\n",
      "tensor(3314)\n",
      "왈레스기념침례병원\n",
      "tensor(3149)\n",
      "부산금정경찰서\n",
      "tensor(3011)\n",
      "베스티안부산병원\n",
      "tensor(2823)\n",
      "가야119안전센터\n",
      "tensor(3247)\n",
      "성신항운외과\n",
      "tensor(3158)\n",
      "부산본병원\n",
      "tensor(2977)\n",
      "만덕지구대\n",
      "tensor(3571)\n",
      "춘해병원\n",
      "tensor(3244)\n",
      "서울유병원\n",
      "tensor(2969)\n",
      "동아대학교병원\n",
      "tensor(3415)\n",
      "주례자연병원\n",
      "tensor(3439)\n",
      "중앙U병원\n",
      "tensor(3409)\n",
      "좋은강안병원\n",
      "tensor(2966)\n",
      "동래성모병원\n",
      "tensor(3251)\n",
      "세흥병원\n",
      "tensor(3373)\n",
      "자모여성병원\n",
      "tensor(2986)\n",
      "미래병원\n",
      "tensor(2609)\n",
      "8468\n",
      "tensor(3283)\n",
      "엘리움병원\n",
      "tensor(3411)\n",
      "좋은미래정형외과의원\n",
      "tensor(3414)\n",
      "주례119안전센터\n"
     ]
    }
   ],
   "source": [
    "for idx in s_ids:\n",
    "    print(idx)\n",
    "    print(ent_ids[int(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "582e86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_example_on_TransE(kg, ent_name=\"부곡지구대\"):\n",
    "    e1 = emb_entity_[kg.ent2ix[ent_name]]\n",
    "    r1 = emb_entity_[kg.rel2ix['koad:roadNameCode']]\n",
    "    e2 = emb_entity_[kg.head_idx]\n",
    "    similarity = cosine_similarity(torch.Tensor([e1]), torch.Tensor(e2))\n",
    "    s_value, s_ids = torch.topk(similarity, 10)\n",
    "    for idx in s_ids:\n",
    "        print(int(idx))\n",
    "        print(kg.head_idx[int(idx)])\n",
    "        print(kg.ix2ent[int(kg.head_idx[int(idx)])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d1e6b",
   "metadata": {},
   "source": [
    "## text to KB entity module\n",
    " - input: text (entity name)\n",
    " - output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d5c67cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDiveEmb(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim):\n",
    "        super(KLDiveEmb, self).__init__()\n",
    "\n",
    "        self.bert = transformers.XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.drop_out = nn.Dropout(0.33)\n",
    "        self.fc1 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, idx, mask):\n",
    "    \n",
    "        x = self.bert(idx, mask) \n",
    "        x = x.pooler_output\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.drop_out(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        return F.softmax(x, dim=1), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "80e38e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, data, max_token):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token = max_token \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        e_name = self.data['e_name'][index] #Entity Name\n",
    "        e_emb = self.data['e_emb'][index]  #KB embedding\n",
    "            \n",
    "        tokenized = self.tokenizer.encode_plus(\"\".join(e_name),\n",
    "                                                None,\n",
    "                                                add_special_tokens=True,\n",
    "                                                max_length = self.max_token,\n",
    "                                                padding='max_length',\n",
    "                                                truncation=True,\n",
    "                                              )\n",
    "        \n",
    "        ids = tokenized['input_ids']\n",
    "        mask = tokenized['attention_mask']\n",
    "\n",
    "        return {'e_ids': torch.tensor(ids, dtype=torch.long), \n",
    "                'e_mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'target': torch.tensor(e_emb)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b4a33ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = KLDiveEmb(768, 512, 256).to(DEVICE)\n",
    "criterion = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005)\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1e5d6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "k,i,v = [],[],[]\n",
    "for ent, idx in ent_vocab.items():\n",
    "    ent_name=ent\n",
    "    e1_ids = ent_vocab[ent_name]\n",
    "    e1_torch = ent_emb(torch.LongTensor([e1_ids]))\n",
    "    k.append(ent)\n",
    "    i.append(idx)\n",
    "    v.append(e1_torch.squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "07306a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['e_name','e_idx', 'e_emb'])\n",
    "df[\"e_name\"] = k\n",
    "df[\"e_idx\"] = i\n",
    "df[\"e_emb\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4a1236d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size=0.2)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "89df0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KLEDataset(tokenizer, train_df, max_token=20)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=4, shuffle=True, pin_memory=True)\n",
    "valid_dataset = KLEDataset(tokenizer, valid_df, max_token=20) \n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size , num_workers=4, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "55d531ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    batch_loss = []\n",
    "    for idx, batch in tqdm(enumerate(train_loader), total=len(train_loader), leave=False): #학습 데이터를 batch size만큼씩 읽어옴\n",
    "        optimizer.zero_grad()\n",
    "        e_ids = batch['e_ids'].to(DEVICE) #entity_name\n",
    "        e_bert_mask = batch['e_mask'].to(DEVICE) \n",
    "        \n",
    "        logits, _ = model(e_ids, e_bert_mask) \n",
    "                \n",
    "        target = batch['target'].to(DEVICE) #KB embedding\n",
    "        target = F.softmax(target, dim=1)\n",
    "        target.requires_grad = False\n",
    "        loss = criterion(logits.log(), target)\n",
    "\n",
    "        batch_loss.append(float(loss.item()))\n",
    "        loss.backward(loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "    return sum(batch_loss) / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9c97426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    batch_loss = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(valid_loader), total=len(valid_loader), leave=False): #학습 데이터를 batch size만큼씩 읽어옴\n",
    "            optimizer.zero_grad()\n",
    "            e_ids = batch['e_ids'].to(DEVICE) #entity_name\n",
    "            e_bert_mask = batch['e_mask'].to(DEVICE) \n",
    "            \n",
    "            logits, _ = model(e_ids, e_bert_mask) \n",
    "                    \n",
    "            target = batch['target'].to(DEVICE) #KB embedding\n",
    "            target = F.softmax(target, dim=1)\n",
    "            loss = criterion(logits.log(), target)\n",
    "\n",
    "            batch_loss.append(float(loss.item()))\n",
    "        \n",
    "    return sum(batch_loss) / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1084ae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010727933085365022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007536372583498188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000720258436829565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000706694227583865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006988779146303522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "for epoch in range(5):\n",
    "    train_avg_loss = train_fn(model, train_loader, optimizer, criterion)\n",
    "    train_epoch_loss.append(train_avg_loss)\n",
    "    print(train_avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "977ecb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006897887845601266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "valid_avg_loss = valid_fn(model, valid_loader, criterion)\n",
    "print(valid_avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5abddef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, keyword=\"춘천\"):\n",
    "\n",
    "    e_name = keyword\n",
    "    tokenized = tokenizer.encode_plus(\"\".join(e_name),\n",
    "                                        None,\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length = 20,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        )\n",
    "\n",
    "    ids = tokenized['input_ids']\n",
    "    mask = tokenized['attention_mask']\n",
    "\n",
    "    batch = {'e_ids': torch.tensor(ids, dtype=torch.long), \n",
    "             'e_mask': torch.tensor(mask, dtype=torch.long)}\n",
    "\n",
    "    e_ids = batch['e_ids'].to(DEVICE).unsqueeze(0) #entity_name\n",
    "    e_bert_mask = batch['e_mask'].to(DEVICE).unsqueeze(0) \n",
    "    logits, word_emb = model(e_ids, e_bert_mask)\n",
    "    e1_torch = word_emb.cpu()\n",
    "    e2_ids = np.arange(0, len(ent_vocab))\n",
    "    e2_torch = ent_emb(torch.LongTensor([e2_ids])).squeeze()\n",
    "    similarity = cosine_similarity(e1_torch, e2_torch)\n",
    "    s_value, s_ids = torch.topk(similarity, 10)\n",
    "    s_ids = s_ids.squeeze()\n",
    "    s_ids\n",
    "\n",
    "    return s_ids.detach().numpy(), s_value.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e226f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_ex_matching(df, query = \"<unk>\"):\n",
    "    keyword = query\n",
    "    exist_boolean = (df[\"e_name\"] == keyword).any()\n",
    "    ex_matching = 0 #기본적으로 \"<unk>\"로 해둠\n",
    "    if exist_boolean: \n",
    "        ex_matching = int(df[df[\"e_name\"] == keyword]['e_idx']) \n",
    "    else: \n",
    "        ex_matching = 0\n",
    "\n",
    "    return ex_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6c3989fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_sim(df, query):\n",
    "    #idx = df[\"e_name\"].apply(lambda x: levenshtein(x, query)).idxmin()\n",
    "    idx = df[\"e_name\"].apply(lambda x: jamo_levenshtein(x, query)).idxmin()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f0e44c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_ent_sim(ent_vocab, ent_emb, query, topN=5):\n",
    "    ent_name=query\n",
    "    e1_ids = ent_vocab[ent_name]\n",
    "    e1_torch = ent_emb(torch.LongTensor([e1_ids]))\n",
    "    e2_ids = np.arange(0, len(ent_vocab))\n",
    "    e2_torch = ent_emb(torch.LongTensor([e2_ids])).squeeze()\n",
    "    similarity = cosine_similarity(e1_torch, e2_torch)\n",
    "    s_value, s_ids = torch.topk(similarity, topN)\n",
    "    s_ids = s_ids.squeeze()\n",
    "    return s_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "65f3defd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Graph similarity 유사도 결과 ----------\n",
      "유사 Entity:  tensor(3562)   참병원\n",
      "유사 Entity:  tensor(3278)   신평파출소\n",
      "유사 Entity:  tensor(2912)   녹산파출소\n",
      "유사 Entity:  tensor(2921)   다대해양파출소\n",
      "유사 Entity:  tensor(3312)   온천3파출소\n",
      "---------- Graph similarity 유사도 끝 ----------\n",
      "\n",
      "\n",
      "---------- BERT-Entity 유사도 결과 시작 ----------\n",
      "[[1749, '7653']]\n",
      "[[1784, '7688']]\n",
      "[[2177, '8051']]\n",
      "[[3167, '부산영도경찰서']]\n",
      "[[2601, '8460']]\n",
      "[[2427, '8294']]\n",
      "[[1853, '7754']]\n",
      "[[2099, '7995']]\n",
      "[[3657, '한빛병원']]\n",
      "[[2653, '8511']]\n",
      "---------- BERT-Entity 유사도 결과 끝 ----------\n"
     ]
    }
   ],
   "source": [
    "query = \"병원\"\n",
    "\n",
    "#EXmatching 결과\n",
    "ex_ids = search_ex_matching(df, query)\n",
    "ex_sim_query = df[df['e_idx'] == int(ex_ids)][\"e_name\"].iloc[0]\n",
    "\n",
    "#Levenshtein 유사도 결과\n",
    "lev_ids = levenshtein_sim(df, query)\n",
    "lev_sim_query = df[df['e_idx'] == int(lev_ids)][\"e_name\"].iloc[0]\n",
    "\n",
    "print(\"---------- Graph similarity 유사도 결과 ----------\")\n",
    "if ex_ids != 0:\n",
    "    graph_ids = graph_ent_sim(ent_vocab, ent_emb, ex_sim_query)\n",
    "else:\n",
    "    graph_ids = graph_ent_sim(ent_vocab, ent_emb, lev_sim_query)\n",
    "\n",
    "\n",
    "for idx in graph_ids:\n",
    "    print(\"유사 Entity: \", idx, \" \", df.iloc[int(idx)][\"e_name\"])\n",
    "print(\"---------- Graph similarity 유사도 끝 ----------\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#BERT-Entity 유사도 결과\n",
    "print(\"---------- BERT-Entity 유사도 결과 시작 ----------\")\n",
    "s_ids, s_score = prediction(model, query)\n",
    "total_ids = s_ids #np.concatenate([np.array([ex_ids]), np.array([lev_ids]), s_ids]) \n",
    "for seq_num, idx in enumerate(total_ids):\n",
    "    print(df[df['e_idx'] == int(idx)][[\"e_idx\",\"e_name\"]].values.tolist())\n",
    "print(\"---------- BERT-Entity 유사도 결과 끝 ----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145a934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d732f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505d93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
